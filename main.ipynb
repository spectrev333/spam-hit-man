{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAM Hit Man\n",
    "\n",
    "Naive bayes approach to spam detection\n",
    "\n",
    "[Dataset](https://github.com/MWiechmann/enron_spam_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df_spam = pl.read_csv(\"enron_spam_data.csv\")\n",
    "\n",
    "df_spam = df_spam.filter(pl.col(\"Message\").is_not_null())\n",
    "\n",
    "df_spam = df_spam.sample(fraction=1, shuffle=True)\n",
    "\n",
    "test_size = 5000\n",
    "\n",
    "df_test, df_train = df_spam.head(test_size), df_spam.tail(-test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common words\n",
    "\n",
    "In the naive bayes approach the presence of a word is a Bernoulli variable.\n",
    "Each one of them conditionaly indipendent from one another, given the evidence.\n",
    "\n",
    "The evidence in this case is also referred to as the ground truth.\n",
    "\n",
    "The 'spam', 'ham' classification is the ground truth\n",
    "\n",
    "Here we take top 500 most common words to build the bayesian network, every word becomes a variable in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = pl.Series(\n",
    "    ' '.join(df_train['Message'].str.to_lowercase()).split()\n",
    ").value_counts()\n",
    "\n",
    "most_common_words = most_common_words.filter(pl.col('').str.contains(r'^\\w+$'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (500,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th></th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;the&quot;</td></tr><tr><td>&quot;to&quot;</td></tr><tr><td>&quot;and&quot;</td></tr><tr><td>&quot;of&quot;</td></tr><tr><td>&quot;a&quot;</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;customer&quot;</td></tr><tr><td>&quot;k&quot;</td></tr><tr><td>&quot;communications&quot;</td></tr><tr><td>&quot;90&quot;</td></tr><tr><td>&quot;events&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (500,)\n",
       "Series: '' [str]\n",
       "[\n",
       "\t\"the\"\n",
       "\t\"to\"\n",
       "\t\"and\"\n",
       "\t\"of\"\n",
       "\t\"a\"\n",
       "\t…\n",
       "\t\"customer\"\n",
       "\t\"k\"\n",
       "\t\"communications\"\n",
       "\t\"90\"\n",
       "\t\"events\"\n",
       "]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words['']\n",
    "top_words = most_common_words.sort('count', descending=True)[:500]['']\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training the model\n",
    "\n",
    "idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_training_dataset = df_train.with_columns(\n",
    "    pl.col('Message').str.contains(f\"{word}\").cast(pl.Int32).alias(word) for word in top_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (28_345, 505)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Message ID</th><th>Subject</th><th>Message</th><th>Spam/Ham</th><th>Date</th><th>the</th><th>to</th><th>and</th><th>of</th><th>a</th><th>in</th><th>for</th><th>you</th><th>is</th><th>_</th><th>this</th><th>enron</th><th>on</th><th>that</th><th>i</th><th>s</th><th>with</th><th>be</th><th>your</th><th>we</th><th>as</th><th>from</th><th>have</th><th>will</th><th>it</th><th>are</th><th>ect</th><th>or</th><th>at</th><th>by</th><th>not</th><th>our</th><th>&hellip;</th><th>director</th><th>july</th><th>special</th><th>schedule</th><th>conference</th><th>low</th><th>employees</th><th>once</th><th>v</th><th>issues</th><th>reply</th><th>texas</th><th>rights</th><th>north</th><th>lon</th><th>using</th><th>another</th><th>move</th><th>stop</th><th>quarter</th><th>check</th><th>ees</th><th>soon</th><th>numbers</th><th>meter</th><th>regarding</th><th>sure</th><th>issue</th><th>et</th><th>states</th><th>mmbtu</th><th>less</th><th>customer</th><th>k</th><th>communications</th><th>90</th><th>events</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>&hellip;</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td></tr></thead><tbody><tr><td>18401</td><td>&quot;order prescriptions directly f…</td><td>&quot;order prescriptions directly f…</td><td>&quot;spam&quot;</td><td>&quot;2004-02-22&quot;</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>30751</td><td>&quot;your reliable pharmacy at the …</td><td>&quot;save more on your prescription…</td><td>&quot;spam&quot;</td><td>&quot;2004-12-11&quot;</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>14651</td><td>&quot;re : enrononline patent applic…</td><td>&quot;netco . mark\n",
       "mark haedicke&quot;</td><td>&quot;ham&quot;</td><td>&quot;2002-01-08&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>10817</td><td>&quot;more then 70 great pornstars s…</td><td>&quot;come explore the world &#x27; s lar…</td><td>&quot;spam&quot;</td><td>&quot;2005-07-18&quot;</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>4783</td><td>&quot;you are our lucky winner ! !&quot;</td><td>&quot;de national lottery\n",
       "po box 101…</td><td>&quot;spam&quot;</td><td>&quot;2005-02-26&quot;</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>&hellip;</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>7384</td><td>&quot;message from charles shen at w…</td><td>&quot;dear vince :\n",
       "how are you ? tha…</td><td>&quot;ham&quot;</td><td>&quot;2000-10-25&quot;</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>5626</td><td>&quot;re : trip to san francisco 3 /…</td><td>&quot;bryan ,\n",
       "i talked to vasant abo…</td><td>&quot;ham&quot;</td><td>&quot;2000-03-01&quot;</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td></tr><tr><td>9652</td><td>&quot;free step - by - step seminar …</td><td>&quot;affluent senior lead program\n",
       "m…</td><td>&quot;spam&quot;</td><td>&quot;2002-06-05&quot;</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>&hellip;</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>28749</td><td>&quot;oneok letter&quot;</td><td>&quot;attached is a revised draft of…</td><td>&quot;ham&quot;</td><td>&quot;2001-08-07&quot;</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>6257</td><td>&quot;re : seneca lake storage&quot;</td><td>&quot;deirdre ,\n",
       "i run two senarios u…</td><td>&quot;ham&quot;</td><td>&quot;2000-06-14&quot;</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>&hellip;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (28_345, 505)\n",
       "┌────────────┬────────────────┬────────────────┬──────────┬───┬─────┬───────────────┬─────┬────────┐\n",
       "│ Message ID ┆ Subject        ┆ Message        ┆ Spam/Ham ┆ … ┆ k   ┆ communication ┆ 90  ┆ events │\n",
       "│ ---        ┆ ---            ┆ ---            ┆ ---      ┆   ┆ --- ┆ s             ┆ --- ┆ ---    │\n",
       "│ i64        ┆ str            ┆ str            ┆ str      ┆   ┆ i32 ┆ ---           ┆ i32 ┆ i32    │\n",
       "│            ┆                ┆                ┆          ┆   ┆     ┆ i32           ┆     ┆        │\n",
       "╞════════════╪════════════════╪════════════════╪══════════╪═══╪═════╪═══════════════╪═════╪════════╡\n",
       "│ 18401      ┆ order          ┆ order          ┆ spam     ┆ … ┆ 1   ┆ 0             ┆ 0   ┆ 0      │\n",
       "│            ┆ prescriptions  ┆ prescriptions  ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│            ┆ directly f…    ┆ directly f…    ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│ 30751      ┆ your reliable  ┆ save more on   ┆ spam     ┆ … ┆ 1   ┆ 0             ┆ 0   ┆ 0      │\n",
       "│            ┆ pharmacy at    ┆ your           ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│            ┆ the …          ┆ prescription…  ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│ 14651      ┆ re :           ┆ netco . mark   ┆ ham      ┆ … ┆ 1   ┆ 0             ┆ 0   ┆ 0      │\n",
       "│            ┆ enrononline    ┆ mark haedicke  ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│            ┆ patent applic… ┆                ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│ 10817      ┆ more then 70   ┆ come explore   ┆ spam     ┆ … ┆ 1   ┆ 0             ┆ 0   ┆ 0      │\n",
       "│            ┆ great          ┆ the world ' s  ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│            ┆ pornstars s…   ┆ lar…           ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│ 4783       ┆ you are our    ┆ de national    ┆ spam     ┆ … ┆ 1   ┆ 0             ┆ 1   ┆ 0      │\n",
       "│            ┆ lucky winner ! ┆ lottery        ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│            ┆ !              ┆ po box 101…    ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│ …          ┆ …              ┆ …              ┆ …        ┆ … ┆ …   ┆ …             ┆ …   ┆ …      │\n",
       "│ 7384       ┆ message from   ┆ dear vince :   ┆ ham      ┆ … ┆ 1   ┆ 0             ┆ 0   ┆ 0      │\n",
       "│            ┆ charles shen   ┆ how are you ?  ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│            ┆ at w…          ┆ tha…           ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│ 5626       ┆ re : trip to   ┆ bryan ,        ┆ ham      ┆ … ┆ 1   ┆ 0             ┆ 1   ┆ 0      │\n",
       "│            ┆ san francisco  ┆ i talked to    ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│            ┆ 3 /…           ┆ vasant abo…    ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│ 9652       ┆ free step - by ┆ affluent       ┆ spam     ┆ … ┆ 1   ┆ 0             ┆ 0   ┆ 0      │\n",
       "│            ┆ - step seminar ┆ senior lead    ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│            ┆ …              ┆ program        ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│            ┆                ┆ m…             ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│ 28749      ┆ oneok letter   ┆ attached is a  ┆ ham      ┆ … ┆ 1   ┆ 0             ┆ 0   ┆ 0      │\n",
       "│            ┆                ┆ revised draft  ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│            ┆                ┆ of…            ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│ 6257       ┆ re : seneca    ┆ deirdre ,      ┆ ham      ┆ … ┆ 1   ┆ 0             ┆ 0   ┆ 0      │\n",
       "│            ┆ lake storage   ┆ i run two      ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "│            ┆                ┆ senarios u…    ┆          ┆   ┆     ┆               ┆     ┆        │\n",
       "└────────────┴────────────────┴────────────────┴──────────┴───┴─────┴───────────────┴─────┴────────┘"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 501)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Spam/Ham</th><th>count_the</th><th>count_to</th><th>count_and</th><th>count_of</th><th>count_a</th><th>count_in</th><th>count_for</th><th>count_you</th><th>count_is</th><th>count__</th><th>count_this</th><th>count_enron</th><th>count_on</th><th>count_that</th><th>count_i</th><th>count_s</th><th>count_with</th><th>count_be</th><th>count_your</th><th>count_we</th><th>count_as</th><th>count_from</th><th>count_have</th><th>count_will</th><th>count_it</th><th>count_are</th><th>count_ect</th><th>count_or</th><th>count_at</th><th>count_by</th><th>count_not</th><th>count_our</th><th>count_if</th><th>count_com</th><th>count_company</th><th>count_1</th><th>&hellip;</th><th>count_director</th><th>count_july</th><th>count_special</th><th>count_schedule</th><th>count_conference</th><th>count_low</th><th>count_employees</th><th>count_once</th><th>count_v</th><th>count_issues</th><th>count_reply</th><th>count_texas</th><th>count_rights</th><th>count_north</th><th>count_lon</th><th>count_using</th><th>count_another</th><th>count_move</th><th>count_stop</th><th>count_quarter</th><th>count_check</th><th>count_ees</th><th>count_soon</th><th>count_numbers</th><th>count_meter</th><th>count_regarding</th><th>count_sure</th><th>count_issue</th><th>count_et</th><th>count_states</th><th>count_mmbtu</th><th>count_less</th><th>count_customer</th><th>count_k</th><th>count_communications</th><th>count_90</th><th>count_events</th></tr><tr><td>str</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>&hellip;</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td></tr></thead><tbody><tr><td>&quot;ham&quot;</td><td>11705</td><td>11821</td><td>10498</td><td>9296</td><td>13913</td><td>12714</td><td>11193</td><td>9287</td><td>11983</td><td>1022</td><td>7546</td><td>6296</td><td>12553</td><td>6555</td><td>13833</td><td>13872</td><td>6953</td><td>10388</td><td>4939</td><td>10012</td><td>11647</td><td>6693</td><td>7474</td><td>6920</td><td>10654</td><td>7429</td><td>8949</td><td>12501</td><td>12796</td><td>5726</td><td>5954</td><td>8261</td><td>8531</td><td>7421</td><td>1342</td><td>10814</td><td>&hellip;</td><td>908</td><td>845</td><td>730</td><td>2221</td><td>821</td><td>4781</td><td>659</td><td>1647</td><td>12478</td><td>1059</td><td>324</td><td>926</td><td>255</td><td>1131</td><td>2582</td><td>877</td><td>796</td><td>1124</td><td>591</td><td>391</td><td>820</td><td>1479</td><td>940</td><td>561</td><td>766</td><td>1108</td><td>2283</td><td>1777</td><td>10623</td><td>345</td><td>623</td><td>1102</td><td>897</td><td>12625</td><td>421</td><td>1041</td><td>289</td></tr><tr><td>&quot;spam&quot;</td><td>11131</td><td>12640</td><td>10784</td><td>10340</td><td>14207</td><td>13568</td><td>9801</td><td>10717</td><td>12305</td><td>1477</td><td>6219</td><td>1</td><td>13181</td><td>5063</td><td>14250</td><td>14166</td><td>6390</td><td>10933</td><td>8136</td><td>9814</td><td>11082</td><td>5167</td><td>5956</td><td>4256</td><td>12390</td><td>7783</td><td>6318</td><td>13079</td><td>12579</td><td>4756</td><td>6131</td><td>10200</td><td>7431</td><td>8852</td><td>2101</td><td>7857</td><td>&hellip;</td><td>741</td><td>247</td><td>1735</td><td>92</td><td>59</td><td>4459</td><td>77</td><td>1434</td><td>13520</td><td>139</td><td>1192</td><td>139</td><td>326</td><td>501</td><td>2712</td><td>829</td><td>449</td><td>2297</td><td>1684</td><td>321</td><td>1274</td><td>701</td><td>790</td><td>556</td><td>263</td><td>388</td><td>1736</td><td>541</td><td>10940</td><td>959</td><td>0</td><td>1633</td><td>1447</td><td>12744</td><td>251</td><td>1224</td><td>596</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 501)\n",
       "┌──────────┬───────────┬──────────┬───────────┬───┬─────────┬─────────────┬──────────┬─────────────┐\n",
       "│ Spam/Ham ┆ count_the ┆ count_to ┆ count_and ┆ … ┆ count_k ┆ count_commu ┆ count_90 ┆ count_event │\n",
       "│ ---      ┆ ---       ┆ ---      ┆ ---       ┆   ┆ ---     ┆ nications   ┆ ---      ┆ s           │\n",
       "│ str      ┆ i32       ┆ i32      ┆ i32       ┆   ┆ i32     ┆ ---         ┆ i32      ┆ ---         │\n",
       "│          ┆           ┆          ┆           ┆   ┆         ┆ i32         ┆          ┆ i32         │\n",
       "╞══════════╪═══════════╪══════════╪═══════════╪═══╪═════════╪═════════════╪══════════╪═════════════╡\n",
       "│ ham      ┆ 11705     ┆ 11821    ┆ 10498     ┆ … ┆ 12625   ┆ 421         ┆ 1041     ┆ 289         │\n",
       "│ spam     ┆ 11131     ┆ 12640    ┆ 10784     ┆ … ┆ 12744   ┆ 251         ┆ 1224     ┆ 596         │\n",
       "└──────────┴───────────┴──────────┴───────────┴───┴─────────┴─────────────┴──────────┴─────────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = parameter_training_dataset.group_by('Spam/Ham').agg(\n",
    "    pl.sum(word).alias(f'count_{word}') for word in top_words\n",
    ")\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 1_001)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Spam/Ham</th><th>count_the</th><th>count_to</th><th>count_and</th><th>count_of</th><th>count_a</th><th>count_in</th><th>count_for</th><th>count_you</th><th>count_is</th><th>count__</th><th>count_this</th><th>count_enron</th><th>count_on</th><th>count_that</th><th>count_i</th><th>count_s</th><th>count_with</th><th>count_be</th><th>count_your</th><th>count_we</th><th>count_as</th><th>count_from</th><th>count_have</th><th>count_will</th><th>count_it</th><th>count_are</th><th>count_ect</th><th>count_or</th><th>count_at</th><th>count_by</th><th>count_not</th><th>count_our</th><th>count_if</th><th>count_com</th><th>count_company</th><th>count_1</th><th>&hellip;</th><th>director</th><th>july</th><th>special</th><th>schedule</th><th>conference</th><th>low</th><th>employees</th><th>once</th><th>v</th><th>issues</th><th>reply</th><th>texas</th><th>rights</th><th>north</th><th>lon</th><th>using</th><th>another</th><th>move</th><th>stop</th><th>quarter</th><th>check</th><th>ees</th><th>soon</th><th>numbers</th><th>meter</th><th>regarding</th><th>sure</th><th>issue</th><th>et</th><th>states</th><th>mmbtu</th><th>less</th><th>customer</th><th>k</th><th>communications</th><th>90</th><th>events</th></tr><tr><td>str</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;ham&quot;</td><td>11705</td><td>11821</td><td>10498</td><td>9296</td><td>13913</td><td>12714</td><td>11193</td><td>9287</td><td>11983</td><td>1022</td><td>7546</td><td>6296</td><td>12553</td><td>6555</td><td>13833</td><td>13872</td><td>6953</td><td>10388</td><td>4939</td><td>10012</td><td>11647</td><td>6693</td><td>7474</td><td>6920</td><td>10654</td><td>7429</td><td>8949</td><td>12501</td><td>12796</td><td>5726</td><td>5954</td><td>8261</td><td>8531</td><td>7421</td><td>1342</td><td>10814</td><td>&hellip;</td><td>0.065087</td><td>0.060576</td><td>0.052341</td><td>0.159101</td><td>0.058857</td><td>0.342403</td><td>0.047258</td><td>0.118001</td><td>0.893527</td><td>0.075899</td><td>0.023271</td><td>0.066375</td><td>0.01833</td><td>0.081054</td><td>0.184949</td><td>0.062867</td><td>0.057067</td><td>0.080553</td><td>0.042389</td><td>0.028068</td><td>0.058786</td><td>0.105972</td><td>0.067378</td><td>0.040241</td><td>0.054919</td><td>0.079407</td><td>0.16354</td><td>0.127309</td><td>0.760705</td><td>0.024774</td><td>0.04468</td><td>0.078978</td><td>0.064299</td><td>0.904053</td><td>0.030216</td><td>0.07461</td><td>0.020765</td></tr><tr><td>&quot;spam&quot;</td><td>11131</td><td>12640</td><td>10784</td><td>10340</td><td>14207</td><td>13568</td><td>9801</td><td>10717</td><td>12305</td><td>1477</td><td>6219</td><td>1</td><td>13181</td><td>5063</td><td>14250</td><td>14166</td><td>6390</td><td>10933</td><td>8136</td><td>9814</td><td>11082</td><td>5167</td><td>5956</td><td>4256</td><td>12390</td><td>7783</td><td>6318</td><td>13079</td><td>12579</td><td>4756</td><td>6131</td><td>10200</td><td>7431</td><td>8852</td><td>2101</td><td>7857</td><td>&hellip;</td><td>0.051589</td><td>0.017243</td><td>0.120698</td><td>0.006466</td><td>0.004172</td><td>0.310088</td><td>0.005423</td><td>0.099771</td><td>0.940068</td><td>0.009734</td><td>0.082945</td><td>0.009734</td><td>0.022735</td><td>0.034902</td><td>0.188625</td><td>0.057707</td><td>0.031287</td><td>0.159772</td><td>0.117152</td><td>0.022388</td><td>0.088646</td><td>0.048808</td><td>0.054995</td><td>0.038726</td><td>0.018355</td><td>0.027046</td><td>0.120768</td><td>0.037683</td><td>0.76069</td><td>0.066745</td><td>0.00007</td><td>0.113606</td><td>0.100674</td><td>0.886116</td><td>0.017521</td><td>0.08517</td><td>0.041507</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 1_001)\n",
       "┌──────────┬───────────┬──────────┬───────────┬───┬──────────┬────────────────┬─────────┬──────────┐\n",
       "│ Spam/Ham ┆ count_the ┆ count_to ┆ count_and ┆ … ┆ k        ┆ communications ┆ 90      ┆ events   │\n",
       "│ ---      ┆ ---       ┆ ---      ┆ ---       ┆   ┆ ---      ┆ ---            ┆ ---     ┆ ---      │\n",
       "│ str      ┆ i32       ┆ i32      ┆ i32       ┆   ┆ f64      ┆ f64            ┆ f64     ┆ f64      │\n",
       "╞══════════╪═══════════╪══════════╪═══════════╪═══╪══════════╪════════════════╪═════════╪══════════╡\n",
       "│ ham      ┆ 11705     ┆ 11821    ┆ 10498     ┆ … ┆ 0.904053 ┆ 0.030216       ┆ 0.07461 ┆ 0.020765 │\n",
       "│ spam     ┆ 11131     ┆ 12640    ┆ 10784     ┆ … ┆ 0.886116 ┆ 0.017521       ┆ 0.08517 ┆ 0.041507 │\n",
       "└──────────┴───────────┴──────────┴───────────┴───┴──────────┴────────────────┴─────────┴──────────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totals = parameter_training_dataset.group_by('Spam/Ham').len().rename({\"len\": \"total\"})\n",
    "\n",
    "parameter_training_dataset = word_counts.join(totals, on='Spam/Ham')\n",
    "\n",
    "# column naming is not very intuitive, but \"melts\" better\n",
    "# +1 and +2 terms are used for Laplace Smoothing, assuming uniform probability\n",
    "parameter_training_dataset = parameter_training_dataset.with_columns(\n",
    "    (\n",
    "        (pl.col(f\"count_{word}\") + 1) / \n",
    "        (pl.col(\"total\") + 2 )\n",
    "    ).alias(f\"{word}\") for word in top_words\n",
    ")\n",
    "\n",
    "parameter_training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (500, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>word</th><th>ham</th><th>spam</th></tr><tr><td>str</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;the&quot;</td><td>0.838178</td><td>0.773969</td></tr><tr><td>&quot;to&quot;</td><td>0.846484</td><td>0.878885</td></tr><tr><td>&quot;and&quot;</td><td>0.751754</td><td>0.749844</td></tr><tr><td>&quot;of&quot;</td><td>0.665688</td><td>0.718974</td></tr><tr><td>&quot;a&quot;</td><td>0.996277</td><td>0.987833</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;customer&quot;</td><td>0.064299</td><td>0.100674</td></tr><tr><td>&quot;k&quot;</td><td>0.904053</td><td>0.886116</td></tr><tr><td>&quot;communications&quot;</td><td>0.030216</td><td>0.017521</td></tr><tr><td>&quot;90&quot;</td><td>0.07461</td><td>0.08517</td></tr><tr><td>&quot;events&quot;</td><td>0.020765</td><td>0.041507</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (500, 3)\n",
       "┌────────────────┬──────────┬──────────┐\n",
       "│ word           ┆ ham      ┆ spam     │\n",
       "│ ---            ┆ ---      ┆ ---      │\n",
       "│ str            ┆ f64      ┆ f64      │\n",
       "╞════════════════╪══════════╪══════════╡\n",
       "│ the            ┆ 0.838178 ┆ 0.773969 │\n",
       "│ to             ┆ 0.846484 ┆ 0.878885 │\n",
       "│ and            ┆ 0.751754 ┆ 0.749844 │\n",
       "│ of             ┆ 0.665688 ┆ 0.718974 │\n",
       "│ a              ┆ 0.996277 ┆ 0.987833 │\n",
       "│ …              ┆ …        ┆ …        │\n",
       "│ customer       ┆ 0.064299 ┆ 0.100674 │\n",
       "│ k              ┆ 0.904053 ┆ 0.886116 │\n",
       "│ communications ┆ 0.030216 ┆ 0.017521 │\n",
       "│ 90             ┆ 0.07461  ┆ 0.08517  │\n",
       "│ events         ┆ 0.020765 ┆ 0.041507 │\n",
       "└────────────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = parameter_training_dataset.select([\n",
    "    \"Spam/Ham\", *[ f\"{word}\" for word in top_words ]\n",
    "    ])\n",
    "\n",
    "probabilities = probabilities.unpivot(index=\"Spam/Ham\", variable_name=\"word\").pivot(\"Spam/Ham\", index=\"word\")\n",
    "\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model\n",
    "\n",
    "fitting is done, now its time to test the model\n",
    "\n",
    "> Note: apparently writing a funtion that does the prediction is terribly inefficent.\n",
    "> This is probably due to how python handles passig parameters around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "total_emails = df_spam.shape[0]\n",
    "\n",
    "spam_emails = totals.filter(pl.col(\"Spam/Ham\") == \"spam\").select(\"total\").row(0)[0]\n",
    "\n",
    "p_spam = spam_emails/total_emails\n",
    "\n",
    "p_ham = 1 - p_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 5)\n",
      "┌────────────┬───────────────────────────┬─────────────────────────┬──────────┬────────────┐\n",
      "│ Message ID ┆ Subject                   ┆ Message                 ┆ Spam/Ham ┆ Date       │\n",
      "│ ---        ┆ ---                       ┆ ---                     ┆ ---      ┆ ---        │\n",
      "│ i64        ┆ str                       ┆ str                     ┆ str      ┆ str        │\n",
      "╞════════════╪═══════════════════════════╪═════════════════════════╪══════════╪════════════╡\n",
      "│ 25441      ┆ are you ready to get it ? ┆ hello !                 ┆ spam     ┆ 2005-06-24 │\n",
      "│            ┆                           ┆ viagra is the # 1 med … ┆          ┆            │\n",
      "└────────────┴───────────────────────────┴─────────────────────────┴──────────┴────────────┘\n",
      "Ground Truth: spam\n",
      "---\n",
      "Spam Score: -25.85226975054966\n",
      "Ham Score : -31.174482023240977\n",
      "Computed spam probability: 0.9999952380409193\n",
      "Verdict: spam\n"
     ]
    }
   ],
   "source": [
    "email_text = df_test.sample()\n",
    "\n",
    "print(email_text)\n",
    "\n",
    "print(f\"Ground Truth: {email_text[\"Spam/Ham\"][0]}\\n---\")\n",
    "\n",
    "words = pl.Series(\n",
    "    ' '.join(email_text['Message'].str.to_lowercase()).split()\n",
    ")\n",
    "\n",
    "spam_score = np.log(p_spam)\n",
    "\n",
    "ham_score = np.log(p_ham)\n",
    "\n",
    "for word in words:\n",
    "    word_probs = probabilities.filter(pl.col(\"word\") == word)\n",
    "    if not word_probs.is_empty():\n",
    "        P_word_spam = word_probs[\"spam\"][0]\n",
    "        P_word_ham = word_probs[\"ham\"][0]\n",
    "        spam_score += np.log(P_word_spam)\n",
    "        ham_score += np.log(P_word_ham)\n",
    "\n",
    "print(f\"Spam Score: {spam_score}\")\n",
    "print(f\"Ham Score : {ham_score}\")\n",
    "\n",
    "spam_probability = np.pow(10, spam_score) / (np.pow(10, spam_score) + np.pow(10, ham_score))\n",
    "\n",
    "print(f\"Computed spam probability: {spam_probability}\")\n",
    "print(f\"Verdict: {\"spam\" if spam_score > ham_score else \"ham\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy calculation\n",
    "\n",
    "Lets see how good is this\n",
    "\n",
    "> Note: as said before a function that does the prediction runs at `<10 it/s` however the approach below goes up to `50ish it/s` (likely just a skill issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [01:51, 44.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "email_text = df_test.sample()\n",
    "\n",
    "#print(email_text)\n",
    "\n",
    "#print(f\"Ground Truth: {email_text['Spam/Ham'][0]}\\n---\")\n",
    "\n",
    "#predict(probabilities, email_text['Message'].str.to_lowercase().item(), p_spam)\n",
    "\n",
    "num_correct = 0\n",
    "total = len(df_test)\n",
    "\n",
    "#print(f\"Total emails in test set: {total}\")\n",
    "\n",
    "for email in tqdm(df_test.iter_rows()):\n",
    "    ground_truth = email[3]\n",
    "    email_message = email[2].lower()\n",
    "    words = set(email_message.split())\n",
    "    \n",
    "    spam_score = np.log(p_spam)\n",
    "    ham_score = np.log(p_ham)\n",
    "    \n",
    "    for word in words:\n",
    "        word_probs = probabilities.filter(pl.col(\"word\") == word)\n",
    "        if not word_probs.is_empty():\n",
    "            P_word_spam = word_probs[\"spam\"][0]\n",
    "            P_word_ham = word_probs[\"ham\"][0]\n",
    "            spam_score += np.log(P_word_spam)\n",
    "            ham_score += np.log(P_word_ham)\n",
    "    \n",
    "    verdict = \"spam\" if spam_score > ham_score else \"ham\"\n",
    "    if verdict == ground_truth:\n",
    "        num_correct += 1\n",
    "\n",
    "print(f\"Accuracy: {num_correct/total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
